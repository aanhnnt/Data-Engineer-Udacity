{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Logging \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder\\\n",
    "                        .config('spark.jars.repositories', 'https://repos.spark-packages.org/') \\\n",
    "                        .config('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11') \\\n",
    "                        .enableHiveSupport()\\\n",
    "                        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(TimestampType())\n",
    "def to_timestamp_udf(x):\n",
    "    try:\n",
    "        return pd.to_timedelta(x, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    except:\n",
    "        return pd.Timestamp('1900-1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    logging.info('Start processing immigration data')\n",
    "    \n",
    "    # Read immigration data file to dataframe\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark').load(input_data)\n",
    "    \n",
    "    # Change data type of some columns from double to integer\n",
    "    toInt = udf(lambda x: int(x) if x!=None else x, IntegerType())\n",
    "\n",
    "    for colname, coltype in df.dtypes:\n",
    "        if coltype == 'double':\n",
    "            df = df.withColumn(colname, toInt(colname))\n",
    "            \n",
    "    logging.info('Start processing fact_immigration table')\n",
    "    \n",
    "    # Extract columns to create fact_immigration table\n",
    "    fact_immigration_df = df.select('cicid', 'i94port', 'i94addr', 'i94visa', 'i94yr', \\\n",
    "                                    'i94mon', 'i94mode', 'arrdate', 'depdate').distinct()\n",
    "\n",
    "    # Rename columns of fact_immigration table\n",
    "    fact_immigration_df = fact_immigration_df.withColumnRenamed('i94port', 'city_code') \\\n",
    "                                    .withColumnRenamed('i94addr', 'state_code') \\\n",
    "                                    .withColumnRenamed('i94visa', 'visa') \\\n",
    "                                    .withColumnRenamed('i94yr', 'year') \\\n",
    "                                    .withColumnRenamed('i94mon', 'month') \\\n",
    "                                    .withColumnRenamed('i94mode', 'mode') \\\n",
    "                                    .withColumnRenamed('arrdate', 'arrive_date') \\\n",
    "                                    .withColumnRenamed('depdate', 'departure_date') \\\n",
    "\n",
    "    # Drop null records on state_code column\n",
    "    fact_immigration_df = fact_immigration_df.where(col('state_code').isNotNull())\n",
    "\n",
    "    # Add country column to fact_immigration table\n",
    "    fact_immigration_df = fact_immigration_df.withColumn('country', lit('United States'))\n",
    "\n",
    "    # Change date type from SAS to timestamp\n",
    "    fact_immigration_df = fact_immigration_df.withColumn('arrive_date', to_date(to_timestamp_udf(col('arrive_date')))) \\\n",
    "                                    .withColumn('departure_date', to_date(to_timestamp_udf(col('departure_date'))))                                    \n",
    "\n",
    "    # Write fact_immigration table to parquet files and partition by state_code\n",
    "    fact_immigration_df.write.mode('overwrite') \\\n",
    "            .partitionBy('state_code') \\\n",
    "            .parquet(output_data + 'fact_immigration')\n",
    "\n",
    "    # Create view for quality check\n",
    "    fact_immigration_df.createOrReplaceTempView('fact_immigration')\n",
    "    \n",
    "    logging.info('Finish processing fact_immigration table')\n",
    "\n",
    "    logging.info('---------------------------------------')\n",
    "\n",
    "    logging.info('Start processing dim_immigrate_person table')\n",
    "\n",
    "    # Extract columns to create dim_immigrate_person\n",
    "    dim_immigrate_person_df = df.select('cicid', 'i94cit', 'i94res',\\\n",
    "                                    'biryear', 'gender', 'insnum').distinct()\n",
    "    \n",
    "    # Rename columns of dim_immigrate_person table\n",
    "    dim_immigrate_person_df = dim_immigrate_person_df.withColumnRenamed('i94cit', 'citizen_country_code') \\\n",
    "                                    .withColumnRenamed('i94res', 'citizen_state_code') \\\n",
    "                                    .withColumnRenamed('biryear', 'birth_year')\n",
    "\n",
    "    # Write dim_immigrat_person table to parquet files    \n",
    "    dim_immigrate_person_df.write.mode('overwrite') \\\n",
    "            .parquet(output_data + 'dim_immigrate_person')\n",
    "\n",
    "    # Create view for quality check\n",
    "    dim_immigrate_person_df.createOrReplaceTempView('dim_immigrate_person')\n",
    "\n",
    "    logging.info('Finish processing dim_immigrate_person table')\n",
    "\n",
    "    logging.info('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_temperature_data(spark, input_data, output_data):\n",
    "    logging.info('Start processing dim_temperature table')\n",
    "\n",
    "    # Read temperature data file to dataframe\n",
    "    df = spark.read.csv(input_data, header=True)\n",
    "\n",
    "    # Filter data in United States\n",
    "    df = df.where(df['Country'] == 'United States')\n",
    "\n",
    "    # Extract columns to create dim_temperature table\n",
    "    dim_temperature_df = df.select('dt', 'AverageTemperature', 'AverageTemperatureUncertainty' \\\n",
    "                            'City', 'Country').distinct()\n",
    "\n",
    "    # Rename columns of dim_temperature\n",
    "    dim_temperature_df = df.withColumnRenamed('dt', 'time_stamp') \\\n",
    "                                .withColumnRenamed('AverageTemperature', 'avg_temperture') \\\n",
    "                                .withColumnRenamed('AverageTemperatureUncertainty', 'avg_temp_uncertainty') \\\n",
    "                                .withColumnRenamed('City', 'city') \\\n",
    "                                .withColumnRenamed('Country', 'country') \\\n",
    "\n",
    "    # Extract year, month from dt column\n",
    "    dim_temperature_df = dim_temperature_df.withColumn('dt', to_date(col('dt')))\n",
    "    dim_temperature_df = dim_temperature_df.withColumn('year', year(dim_temperature_df['dt']))\n",
    "    dim_temperature_df = dim_temperature_df.withColumn('month', month(dim_temperature_df['dt']))\n",
    "\n",
    "    # Write dim_temperature to parquet files\n",
    "    dim_temperature_df.write.mode('overwrite') \\\n",
    "            .parquet(output_data + 'dim_temperature')\n",
    "\n",
    "    # Create view for quality check\n",
    "    dim_temperature_df.createOrReplaceTempView('dim_temperature')\n",
    "\n",
    "    logging.info('Finish processing dim_temperature table')\n",
    "\n",
    "    logging.info('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_demographic_data(spark, input_data, output_data):\n",
    "    logging.info('Start processing dim_demographic table')\n",
    "\n",
    "    # Read demographic data to dataframe\n",
    "    df = spark.read.csv(input_data).option(header=True, delimiter=';')\n",
    "    \n",
    "    # Rename columns of dim_demographic table\n",
    "    dim_demographic_df  = dim_demographic_df.withColumnRenamed('City','city') \\\n",
    "                                    .withColumnRenamed('State','state') \\\n",
    "                                    .withColumnRenamed('Median Age','median_age') \\\n",
    "                                    .withColumnRenamed('Male Population','male_population') \\\n",
    "                                    .withColumnRenamed('Female Population','female_population') \\\n",
    "                                    .withColumnRenamed('Total Population','total_population') \\\n",
    "                                    .withColumnRenamed('Number of Veterans','number_of_veterans') \\\n",
    "                                    .withColumnRenamed('Foreign-born','foreign_born') \\\n",
    "                                    .withColumnRenamed('Average Household Size','avg_household_hold') \\\n",
    "                                    .withColumnRenamed('State Code','state_code')\\\n",
    "                                    .withColumnRenamed('Race','race') \\\n",
    "                                    .withColumnRenamed('Count','count')\n",
    "    \n",
    "    # Change type of some columns to integer\n",
    "    colnames = ['median_age', 'male_population', 'female_population', 'total_population', \n",
    "                'number_of_veterans', 'foreign_born', 'avg_household_hold', 'count']\n",
    "\n",
    "    for colname in colnames:\n",
    "        dim_demographic_df = dim_demographic_df.withColumn(colname, toInt(colname))\n",
    "\n",
    "    # Write dim_demographic table to parquet files\n",
    "    dim_demographic_df.write.mode('overwrite') \\\n",
    "            .parquet(output_data + 'dim_demographic')\n",
    "    \n",
    "    # Create view for quality check\n",
    "    dim_demographic_df.createOrReplaceTempView('dim_demographic')\n",
    "\n",
    "    logging.info('Finish processing dim_demographic table')\n",
    "\n",
    "    logging.info('---------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_airport_data(spark, input_data, output_data): \n",
    "    logging.info('Start processing dim_airport table')\n",
    "\n",
    "    # Read airport data to dataframe\n",
    "    df = spark.read.csv(input_data, header=True)\n",
    "    \n",
    "    # Convert elevation_ft to type integer\n",
    "    df = df.withColumn('elevation_ft', toInt('elevation_ft'))\n",
    "   \n",
    "    # Add state_code column to join with fact table\n",
    "    df = df.withColumn('state_code', split(col('iso_region'), '-').getItem(1)) \n",
    "    \n",
    "    # Extract columns to create dim_airport table\n",
    "    dim_airport_df = df.select('ident', 'type', 'name', 'elevation_ft','iso_country', \\\n",
    "                                'state_code', 'municipality', 'coordinates')\n",
    "\n",
    "    # Write dim_airport table to parquet files\n",
    "    dim_airport_df.write.mode('overwrite')\\\n",
    "            .parquet(output_data + 'dim_airport')\n",
    "    \n",
    "    # Create view for quality check\n",
    "    dim_airport_df.createOrReplaceTempView('dim_airport')\n",
    "\n",
    "    logging.info('Finish processing dim_airport table')\n",
    "\n",
    "    logging.info('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(spark):\n",
    "    logging.info('Start checking data quality')\n",
    "\n",
    "    tables = ['fact_immigration', 'dim_immigrate_person', 'dim_temperature', 'dim_demographic', 'dim_airport']\n",
    "    for table in tables:\n",
    "        print(f\"Checking data quality on table {table}...\")\n",
    "        expected_result = spark.sql(f\"\"\"SELECT COUNT(*) FROM {table} WHERE state_code IS NULL\"\"\")\n",
    "        if expected_result.head()[0] > 0:\n",
    "            print(f\"Data quality check failed! Found NULL values in {table} table!\")\n",
    "        else:\n",
    "            print(f\"Table {table} passed\")\n",
    "\n",
    "    logging.info('Finish checking data quality')\n",
    "\n",
    "    logging.info('---------------------------------------')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "output_data = \"./output_data\"\n",
    "\n",
    "immigration_data = \"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\"\n",
    "temperature_data = \"../../data2/GlobalLandTemperaturesByCity.csv\"\n",
    "demographic_data = \"./us-cities-demographics.csv\"\n",
    "airport_data = \"./airport-codes_csv.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_immigration_data(spark, immigration_data, output_data)\n",
    "process_temperature_data(spark, temperature_data, output_data)\n",
    "process_demographic_data(spark, demographic_data, output_data)\n",
    "process_airport_data(spark, airport_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore United States Immigration Data\n",
    "    Udacity Data Engineering Nanodegree Capstone Project\n",
    "\n",
    "## Overview\n",
    "The Organization for Tourism Development (OTD) want to analyze migration flux in USA, in order to find insights to significantly and sustainably develop the tourism in USA. To support their core idea they have identified a set of analysis/queries they want to run on the raw data available. The project deals with building a ETL data pipeline, to go from raw data to the data insights on the migration flux.\n",
    "\n",
    "The project includes 5 steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "### Step 1: Scope the Project and Gather Data\n",
    "#### What data \n",
    "This project using 4 datasets includes:\n",
    "* I94 immigration data for year 2016. Used for the main analysis\n",
    "* World Temperature Data\n",
    "* Airport Code Table\n",
    "* U.S. City Demographic Data\n",
    "\n",
    "#### What tools\n",
    "* Apache Hadoop: using to stored data\n",
    "* Apche Spark: using to transform and analyst data\n",
    "* Apache Aiflow: using to schedule tasks\n",
    "\n",
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "Read the file `Capstone Project Template.ipynb`\n",
    "\n",
    "### Step 3: Define the Data Model\n",
    "Conceptual data model using Star schema\n",
    "\n",
    "![diagram](concept_model.png)\n",
    "\n",
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Fact Immigration:\n",
    "- Read immigration data file to dataframe\n",
    "- Change data type of some columns from double to integer\n",
    "- Extract columns to create fact_immigration table\n",
    "- Rename columns of fact_immigration table\n",
    "- Drop null records on state_code column\n",
    "- Add country column to fact_immigration table\n",
    "- Change date type from SAS to timestamp\n",
    "- Write fact_immigration table to parquet files and partition by state_code\n",
    "\n",
    "Dim Immigrate Person:\n",
    "- Extract columns to create dim_immigrate_person from immigration data\n",
    "- Rename columns of dim_immigrate_person table\n",
    "- Write dim_immigrat_person table to parquet files  \n",
    "\n",
    "Dim Temperature\n",
    "- Read temperature data file to dataframe\n",
    "- Filter data in United States\n",
    "- Extract columns to create dim_temperature table\n",
    "- Rename columns of dim_temperature\n",
    "- Extract year, month from dt column\n",
    "- Write dim_temperature to parquet files\n",
    "\n",
    "Dim Demographic\n",
    "- Read demographic data to dataframe\n",
    "- Rename columns of dim_demographic table\n",
    "- Change type of some columns to float\n",
    "- Write dim_demographic table to parquet files\n",
    "\n",
    "Dim Airport\n",
    "- Read airport data to dataframe\n",
    "- Convert elevation_ft to type integer\n",
    "- Add state_code column to join with fact table\n",
    "- Extract columns to create dim_airport table\n",
    "- Write dim_airport table to parquet files\n",
    "\n",
    "Read the file `Capstone Project Template.ipynb`\n",
    "\n",
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Read the file `Capstone Project Template.ipynb`\n",
    "\n",
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "* fact_immigration\n",
    "    - cicid: integer (nullable = true)\n",
    "    - city_code: string (nullable = true)\n",
    "    - state_code: string (nullable = true)\n",
    "    - visa: integer (nullable = true)\n",
    "    - year: integer (nullable = true)\n",
    "    - month: integer (nullable = true)\n",
    "    - mode: integer (nullable = true)\n",
    "    - arrive_date: date (nullable = true)\n",
    "    - departure_date: date (nullable = true)\n",
    "    - country: string (nullable = false)\n",
    "\n",
    "* dim_immigrate_person\n",
    "    - cicid: integer (nullable = true)\n",
    "    - citizen_country_code: integer (nullable = true)\n",
    "    - citizen_state_code: integer (nullable = true)\n",
    "    - birth_year: integer (nullable = true)\n",
    "    - gender: string (nullable = true)\n",
    "    - insnum: string (nullable = true)\n",
    "\n",
    "* dim_temperature\n",
    "    - dt: string (nullable = true)\n",
    "    - avg_temperture: string (nullable = true)\n",
    "    - avg_temp_uncertainty: string (nullable = true)\n",
    "    - city: string (nullable = true)\n",
    "    - country: string (nullable = true)\n",
    "\n",
    "* dim_demographic\n",
    "    - city: string (nullable = true)\n",
    "    - state: string (nullable = true)\n",
    "    - median_age: float (nullable = true)\n",
    "    - male_population: float (nullable = true)\n",
    "    - female_population: float (nullable = true)\n",
    "    - total_population: float (nullable = true)\n",
    "    - number_of_veterans: float (nullable = true)\n",
    "    - foreign_born: float (nullable = true)\n",
    "    - avg_household_size: float (nullable = true)\n",
    "    - state_code: string (nullable = true)\n",
    "    - race: string (nullable = true)\n",
    "    - count: float (nullable = true)\n",
    "\n",
    "\n",
    "* dim_airport\n",
    "    - ident: string (nullable = true)\n",
    "    - type: string (nullable = true)\n",
    "    - name: string (nullable = true)\n",
    "    - elevation_ft: integer (nullable = true)\n",
    "    - iso_country: string (nullable = true)\n",
    "    - state_code: string (nullable = true)\n",
    "    - municipality: string (nullable = true)\n",
    "    - coordinates: string (nullable = true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore United States Immigration Data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this data engineering capstone project is to create an ETL pipeline integrating data from different data sources for data analysis of immigration data for the US. We can explore insights from data and then make better decisions on immigration policies for those who came and will be coming in near future to the US.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project we will be using the following datasets: I94 Immigration Data, World Temperature Data, U.S. City Demographic Data to create data warehouse\n",
    "* I94 Immigration Data: Data contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries).\n",
    "* World Temperature Data: This dataset is from Kaggle and contains monthly average temperature data at different country in the world wide.\n",
    "* U.S. City Demographic Data: This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000.\n",
    "\n",
    "The technology used in this project is:\n",
    "- Apache Hadoop: using to storage data for reading and processed\n",
    "- Apche Spark: using to transform and analyst data\n",
    "- Apache Aiflow: using to schedule tasks\n",
    "\n",
    "### Step 2: Explore and Assess the Data\n",
    "Refer to <mark>Capstine Project Template.ipynb</mark>\n",
    "\n",
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "![diagram](image.png)\n",
    "\n",
    "- The imigration fact table is a center of the model. This table's data comes from the immigration dataset contain state_code link to the dimension tables.\n",
    "- The temperature dimension table look at the temperature data of New York city\n",
    "- The demographic dimension table allow us explore the population of states and Which areas have the most traveller to visit,...\n",
    "- The airport dimension table show us description of areas airport\n",
    "\n",
    "Immigration fact:\n",
    "- read dataset from SAS file\n",
    "- count number of record in df_immigration\n",
    "- drop ducplicates base on cicid\n",
    "- convert arrive_date and departure_date to timestamp\n",
    "- count null with two columns arrive_date and depature_date\n",
    "- check the difference between gender\n",
    "- select columns necessary and rename for fact table\n",
    "- write parquet file to hdfs\n",
    "\n",
    "Demographic dimension:\n",
    "- read dataset from csv file\n",
    "- drop duplicate data\n",
    "- change type of some columns to interger\n",
    "- find top 5 city have the most population\n",
    "- find sum of veterans at Chicago in each state\n",
    "- rename all columns\n",
    "- write parquet file to hdfs\n",
    "\n",
    "Temperature dimension:\n",
    "- read dataset from csv file or hdfs\n",
    "- check null with AverageTemperature and AverageTemperatureUncertainty column\n",
    "- limit data in US country and New York city\n",
    "- check recent date and previsous date\n",
    "- rename columns and add state_code column fill with \"NY\" to join with fact table\n",
    "- write parquet file to hdfs\n",
    "\n",
    "Airport dimension:\n",
    "- read dataset from csv file or hdfs\n",
    "- check null of some columns\n",
    "- count number of types airports\n",
    "- convert elevation_ft to type integer\n",
    "- the average elevation in each airports\n",
    "- select columns necessary and add \"state_code\" column to join with fact table\n",
    "- write parquet file to hdfs\n",
    "\n",
    "### Step 4: Run Pipelines to Model the Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc85bb2ca4a9e22e60bb5a4053386b3366aaa656a8099e17c6a92604338c7282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
