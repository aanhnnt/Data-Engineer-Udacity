{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!unzip ./data/song-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data/log-data.zip\n",
      "  inflating: ./data/log_data/2018-11-01-events.json  \n",
      "  inflating: ./data/log_data/2018-11-02-events.json  \n",
      "  inflating: ./data/log_data/2018-11-03-events.json  \n",
      "  inflating: ./data/log_data/2018-11-04-events.json  \n",
      "  inflating: ./data/log_data/2018-11-05-events.json  \n",
      "  inflating: ./data/log_data/2018-11-06-events.json  \n",
      "  inflating: ./data/log_data/2018-11-07-events.json  \n",
      "  inflating: ./data/log_data/2018-11-08-events.json  \n",
      "  inflating: ./data/log_data/2018-11-09-events.json  \n",
      "  inflating: ./data/log_data/2018-11-10-events.json  \n",
      "  inflating: ./data/log_data/2018-11-11-events.json  \n",
      "  inflating: ./data/log_data/2018-11-12-events.json  \n",
      "  inflating: ./data/log_data/2018-11-13-events.json  \n",
      "  inflating: ./data/log_data/2018-11-14-events.json  \n",
      "  inflating: ./data/log_data/2018-11-15-events.json  \n",
      "  inflating: ./data/log_data/2018-11-16-events.json  \n",
      "  inflating: ./data/log_data/2018-11-17-events.json  \n",
      "  inflating: ./data/log_data/2018-11-18-events.json  \n",
      "  inflating: ./data/log_data/2018-11-19-events.json  \n",
      "  inflating: ./data/log_data/2018-11-20-events.json  \n",
      "  inflating: ./data/log_data/2018-11-21-events.json  \n",
      "  inflating: ./data/log_data/2018-11-22-events.json  \n",
      "  inflating: ./data/log_data/2018-11-23-events.json  \n",
      "  inflating: ./data/log_data/2018-11-24-events.json  \n",
      "  inflating: ./data/log_data/2018-11-25-events.json  \n",
      "  inflating: ./data/log_data/2018-11-26-events.json  \n",
      "  inflating: ./data/log_data/2018-11-27-events.json  \n",
      "  inflating: ./data/log_data/2018-11-28-events.json  \n",
      "  inflating: ./data/log_data/2018-11-29-events.json  \n",
      "  inflating: ./data/log_data/2018-11-30-events.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip ./data/log-data.zip -d ./data/log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!rm -rf ./data/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import (year, month, dayofmonth, hour, weekofyear, \n",
    "                                    date_format, dayofweek, monotonically_increasing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"./data/\"\n",
    "output_data = \"./data/parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(['song_id','title','artist_id','year','duration']) \\\n",
    "                    .dropDuplicates()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy(['year', 'artist_id']) \\\n",
    "                .parquet(\"{}/parquet/songs\".format(output_data), mode=\"overwrite\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(['artist_id', 'artist_name', 'artist_location',\n",
    "                              'artist_latitude', 'artist_longitude']) \\\n",
    "                        .withColumnRenamed('artist_name', 'name') \\\n",
    "                        .withColumnRenamed('artist_location', 'location') \\\n",
    "                        .withColumnRenamed('artist_latitude', 'latitude') \\\n",
    "                        .withColumnRenamed('artist_longitude', 'longitude') \\\n",
    "                        .dropDuplicates() \n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(\"{}/parquet/artists\".format(output_data), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + 'log_data'\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    events_df = df.filter(df.page == 'NextSong') \\\n",
    "                   .select(['ts', 'userId', 'level', 'song', 'artist',\n",
    "                           'length', 'sessionId', 'location', 'userAgent'])\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select(['userId', 'firstName', 'lastName',\n",
    "                            'gender', 'level']).dropDuplicates()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet(\"{}/parquet/users\".format(output_data), mode='overwrite')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: str(int(int(x)/1000)))\n",
    "    events_df = events_df.withColumn('timestamp', get_timestamp(events_df.ts)) \n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000)))\n",
    "    events_df = events_df.withColumn('datetime', get_datetime(events_df.ts)) \\\n",
    "                        .withColumnRenamed('datetime', 'start_time')\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = events_df.select('start_time') \\\n",
    "                           .withColumn('hour', hour('start_time')) \\\n",
    "                           .withColumn('day', dayofmonth('start_time')) \\\n",
    "                           .withColumn('week', weekofyear('start_time')) \\\n",
    "                           .withColumn('month', month('start_time')) \\\n",
    "                           .withColumn('year', year('start_time')) \\\n",
    "                           .withColumn('weekday', dayofweek('start_time')) \\\n",
    "                           .dropDuplicates()\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month') \\\n",
    "                    .parquet(\"{}/parquet/time\".format(output_data), 'overwrite')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    stg_songs_df = spark.read.json(input_data + 'song_data/*/*/*/*.json')\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    joined_df = events_df.join(stg_songs_df, \\\n",
    "                        (events_df.artist == stg_songs_df.artist_name) \\\n",
    "                        & (events_df.song == stg_songs_df.title) \\\n",
    "                        & (events_df.length == stg_songs_df.duration), \\\n",
    "                    how='inner')\n",
    "\n",
    "    songplays_table = joined_df.select(['start_time', 'userId', 'level', 'song_id', \\\n",
    "                                    'artist_id', 'sessionId', 'artist_location', 'userAgent']) \\\n",
    "                                .withColumnRenamed('userId', 'user_id') \\\n",
    "                                .withColumnRenamed('sessionId', 'session_id') \\\n",
    "                                .withColumnRenamed('artist_location', 'location') \\\n",
    "                                .withColumnRenamed('userAgent', 'user_agent') \\\n",
    "                                .dropDuplicates() \\\n",
    "                                .withColumn('year', year('start_time')) \\\n",
    "                                .withColumn('month', month('start_time')) \\\n",
    "                                .withColumn('songplay_id', monotonically_increasing_id())\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.partitionBy(['year', 'month']) \\\n",
    "            .parquet(\"{}/parquet/songplays\".format(output_data), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process_song_data(spark, input_data, output_data)   \n",
    "# print('\\nDone 1\\n')\n",
    "process_log_data(spark, input_data, output_data)\n",
    "print('\\nDone 2\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_data = input_data + 'song_data/**/*.json'\n",
    "log_data = glob.glob(\"data/song_data/**/*.json\", recursive=True)\n",
    "df = spark.read.json(log_data)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Lake with Spark and S3\n",
    "\n",
    "## Introduction\n",
    "A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. This project is responsible for building an ETL pipeline in order to their analytics team to continue finding insights into what songs their users are listening to. The ETL pipeline inlcude:\n",
    "- Extracts their data from S3\n",
    "- Processes them using Spark\n",
    "- Transforms data into S3 as a set of dimensional tables\n",
    "\n",
    "## Data Warehouse schema design\n",
    "- Fact table: songplays\n",
    "- Dimension tables: users, songs, artists, time \n",
    "\n",
    "## How to run the Python Scripts\n",
    "Fistly, you need to copy the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, paste them into `dl.cfg`.\n",
    "After that, run the command\n",
    "```\n",
    "    python etl.py\n",
    "```\n",
    "\n",
    "## Explanation\n",
    "- `dl.cfg` is where contains your AWS credentials\n",
    "- `etl.py` is where you'll reads data from S3, processes that data using Spark, and writes them back to S3.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
